{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Functions\n",
    "\n",
    "We begin by loading the necessary packages/modules and define the following functions:\n",
    "\n",
    "- `concat_excel(directory)`: Merges the data pulled from Refinitiv in the [`tick`](<..\\data\\raw\\tick>) file directory.\n",
    "- `process_summary(directory)`:  Merges the data pulled from Refinitiv in the [`summary`](<..\\data\\raw\\summary>) file directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def concat_excel(directory):\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # loop through all files in the directory\n",
    "    for file in os.listdir(directory):        \n",
    "        if file.endswith('.xlsx'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_excel(file_path, header=6) # change header position\n",
    "            concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "def process_summary(directory):\n",
    "    ticker_data = []\n",
    "\n",
    "    # loop through all files in the directory\n",
    "    for file in os.listdir(directory):        \n",
    "        if file.endswith('.xlsx'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "\n",
    "            # read without headers to find the correct header row\n",
    "            temp_df = pd.read_excel(file_path, header=None)\n",
    "            temp_df = temp_df.astype(str)  # Convert all columns to strings\n",
    "\n",
    "            header_row = temp_df[temp_df.apply(lambda x: x.str.contains('Exchange Date', na=False)).any(axis=1)].index[0]\n",
    "            \n",
    "            # re-read with correct header row\n",
    "            df = pd.read_excel(file_path, header=header_row)\n",
    "\n",
    "            # convert 'Exchange Date' to datetime and filter by date range\n",
    "            df['Exchange Date'] = pd.to_datetime(df['Exchange Date'])\n",
    "            start_date = datetime(2023, 12, 21)\n",
    "            end_date = datetime(2024, 3, 18)\n",
    "            df = df[df['Exchange Date'].between(start_date, end_date)]\n",
    "\n",
    "            # calc avgs\n",
    "            averages = df[['Bid', 'Ask', 'Volume']].mean()\n",
    "            averages.name = file.split('.')[0]  # use the file name as the ticker\n",
    "                        \n",
    "            # append data to list\n",
    "            ticker = file.split('.')[0]\n",
    "            ticker_data.append([ticker, averages['Bid'], averages['Ask'], averages['Volume']])\n",
    "\n",
    "    # convert list of data to a DataFrame\n",
    "    main_df = pd.DataFrame(ticker_data, columns=['Ticker', 'Avg Bid', 'Avg Ask', 'Avg Volume'])\n",
    "\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging intraday tick data\n",
    "\n",
    "We merge the raw tick data (from 21.12.2023 to 18.03.2024) of the three different cohorts (made up of 15 SWX equities separated by liquidity level) in the [`tick`](<..\\data\\raw\\tick>) file directory before proceeding to the final merge into one dataset. The process is separated by cohorts as each merge takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = concat_excel(os.path.join('..', 'data', 'raw', 'tick', 'high'))\n",
    "df.to_csv(os.path.join('..', 'data', 'interim', 'prelim merge', 'merge1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = concat_excel(os.path.join('..', 'data', 'raw', 'tick', 'medium'))\n",
    "df.to_csv(os.path.join('..', 'data', 'interim', 'prelim merge', 'merge2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = concat_excel(os.path.join('..', 'data', 'raw', 'tick', 'low'))\n",
    "df.to_csv(os.path.join('..', 'data', 'interim', 'prelim merge', 'merge3.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diana\\AppData\\Local\\Temp\\ipykernel_19732\\1076590402.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Diana\\AppData\\Local\\Temp\\ipykernel_19732\\1076590402.py:6: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Diana\\AppData\\Local\\Temp\\ipykernel_19732\\1076590402.py:6: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0      RIC                 Timestamp    Tick  Last Trade  Volume  \\\n",
      "0           0   ABBN.S  21-Dec-2023 17:40:16.203  DOWN â†“        37.1     1.0   \n",
      "1           1   ABBN.S  21-Dec-2023 17:40:02.423     NaN         NaN     NaN   \n",
      "2           2   UBSG.S  21-Dec-2023 17:40:02.422     NaN         NaN     NaN   \n",
      "3           3    KUD.S  21-Dec-2023 17:40:02.420     NaN         NaN     NaN   \n",
      "4           4  SRENH.S  21-Dec-2023 17:40:02.419     NaN         NaN     NaN   \n",
      "\n",
      "   VWAP  Bid  Ask          Flow  Calc VWAP Venue  Bid Size  Ask Size  Turnover  \n",
      "0   NaN  NaN  NaN -2.273539e+08  36.886702   NaN       NaN       NaN       NaN  \n",
      "1   NaN  NaN  NaN -2.273538e+08  36.886702   NaN       NaN       NaN       NaN  \n",
      "2   NaN  NaN  NaN -8.667000e+09  26.254416   NaN       NaN       NaN       NaN  \n",
      "3   NaN  NaN  NaN  6.720380e+03   1.202547   NaN       NaN       NaN       NaN  \n",
      "4   NaN  NaN  NaN  1.430147e+09  94.215826   NaN       NaN       NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "dir = os.path.join('..', 'data', 'interim', 'prelim merge')\n",
    "merged_df = pd.DataFrame()\n",
    "for file in os.listdir(dir):        \n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(dir, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "merged_df.to_csv(os.path.join('..', 'data', 'interim', 'merged.csv'), index=False)\n",
    "print(merged_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "We clean the data by removing unnecessary columns and values and deleting rows that have NaN values in 'Last Trade' and/or 'Volume'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and keep desired columns\n",
    "columns = ['RIC', 'Timestamp', 'Tick', 'Last Trade', 'Volume']\n",
    "final_df = merged_df[columns].copy()\n",
    "\n",
    "# remove unnecessary arrow sign in 'Tick' and null values\n",
    "final_df['Tick'] =final_df['Tick'].astype(str).str[:-2]\n",
    "final_df.dropna(subset=['Last Trade', 'Volume'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates\n",
    "\n",
    "We check for duplicates in the columns 'RIC' and 'Timestamp' which indicate simultaneously executed trades and keep the last occurence in each series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates found: 1662427\n"
     ]
    }
   ],
   "source": [
    "duplicates = final_df.duplicated(subset=['RIC', 'Timestamp'])\n",
    "\n",
    "# count duplicates\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicates found: {num_duplicates}\")\n",
    "\n",
    "final_df.drop_duplicates(subset=['RIC', 'Timestamp'], keep='last', inplace=True)\n",
    "final_df.to_csv(os.path.join('..', 'data', 'processed', 'tick_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging issue-level time series data\n",
    "\n",
    "We continue by merging summary statistics and market cap data for our modeling time period (21.12.2023 to 18.03.2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ticker        Avg Bid        Avg Ask    Avg Volume\n",
      "0    ABBN      38.430678      38.443390  3.117397e+06\n",
      "1     CLN      11.331864      11.343051  9.777089e+05\n",
      "2    DOKA     444.076271     444.703390  4.409932e+03\n",
      "3    GIVN    3612.220339    3613.694915  1.869014e+04\n",
      "4     KUD       1.277119       1.304237  6.600285e+04\n",
      "5    LISN  107949.152542  108413.559322  1.026271e+02\n",
      "6    LONN     418.901695     419.050847  2.393532e+05\n",
      "7    NESN      96.595254      96.609492  3.676267e+06\n",
      "8    SCHN     210.284746     210.516949  2.383359e+04\n",
      "9    SCMN     510.150847     510.376271  8.610905e+04\n",
      "10   SIKA     250.783051     250.896610  2.944247e+05\n",
      "11  SRENH     101.622034     101.660678  8.196646e+05\n",
      "12   UBSG      25.525763      25.535932  6.549273e+06\n",
      "13    UHR     211.226271     211.328814  1.652034e+05\n",
      "14   VLRT      11.042241      11.981356  2.366591e+02\n"
     ]
    }
   ],
   "source": [
    "result_df = process_summary(os.path.join('..', 'data', 'raw', 'summary'))\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't bother with creating a custom function to merge MC data. After reading the file, we merge it with our `result_df` dataframe using the ticker as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Identifier                            Company Name        Avg MC Ticker  \\\n",
      "0        ABBN                                 Abb Ltd  7.224162e+10   ABBN   \n",
      "1         CLN                             Clariant AG  3.790858e+09    CLN   \n",
      "2        DOKA                    Dormakaba Holding AG  1.870050e+09   DOKA   \n",
      "3        GIVN                             Givaudan SA  3.331868e+10   GIVN   \n",
      "4         KUD                             Kudelski SA  6.584905e+07    KUD   \n",
      "5        LISN  Chocoladefabriken Lindt & Spruengli AG  1.453010e+10   LISN   \n",
      "6        LONN                          Lonza Group AG  3.091884e+10   LONN   \n",
      "7        NESN                               Nestle SA  2.580040e+11   NESN   \n",
      "8        SCHN                    Schindler Holding AG  1.407383e+10   SCHN   \n",
      "9        SCMN                             Swisscom AG  2.644564e+10   SCMN   \n",
      "10       SIKA                                 Sika AG  4.053589e+10   SIKA   \n",
      "11      SRENH                             Swiss Re AG  3.213975e+10  SRENH   \n",
      "12       UBSG                            UBS Group AG  8.868002e+10   UBSG   \n",
      "13        UHR                         Swatch Group AG  6.146653e+09    UHR   \n",
      "14       VLRT                       Valartis Group AG  3.785213e+07   VLRT   \n",
      "\n",
      "          Avg Bid        Avg Ask    Avg Volume  \n",
      "0       38.430678      38.443390  3.117397e+06  \n",
      "1       11.331864      11.343051  9.777089e+05  \n",
      "2      444.076271     444.703390  4.409932e+03  \n",
      "3     3612.220339    3613.694915  1.869014e+04  \n",
      "4        1.277119       1.304237  6.600285e+04  \n",
      "5   107949.152542  108413.559322  1.026271e+02  \n",
      "6      418.901695     419.050847  2.393532e+05  \n",
      "7       96.595254      96.609492  3.676267e+06  \n",
      "8      210.284746     210.516949  2.383359e+04  \n",
      "9      510.150847     510.376271  8.610905e+04  \n",
      "10     250.783051     250.896610  2.944247e+05  \n",
      "11     101.622034     101.660678  8.196646e+05  \n",
      "12      25.525763      25.535932  6.549273e+06  \n",
      "13     211.226271     211.328814  1.652034e+05  \n",
      "14      11.042241      11.981356  2.366591e+02  \n"
     ]
    }
   ],
   "source": [
    "mc_df = pd.read_excel(os.path.join('..', 'data', 'raw', 'market_cap.xlsx'))\n",
    "mc_df = mc_df[['Identifier', 'Company Name', 'Avg MC']]\n",
    "mc_df['Identifier'] = mc_df['Identifier'].str.replace('.S', '', regex=False)\n",
    "\n",
    "sum_df = pd.merge(mc_df, result_df, left_on='Identifier', right_on='Ticker')\n",
    "print(sum_df)\n",
    "sum_df.to_csv(os.path.join('..', 'data', 'processed', 'summary_data.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riskmanagement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
