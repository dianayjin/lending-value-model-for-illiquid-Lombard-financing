{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Functions\n",
    "\n",
    "We begin by loading the necessary packages/modules and define a fucntion to merge the raw data pulled from Refinitiv in [`\\raw`](<..\\data\\raw>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def concat_excel(directory):\n",
    "    # create an empty DataFrame to hold concatenated data\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # loop through all files in the directory\n",
    "    for file in os.listdir(directory):        \n",
    "        if file.endswith('.xlsx') or file.endswith('.xls'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_excel(file_path, header=6)\n",
    "            concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging datasets\n",
    "\n",
    "We merge the raw data of the three different cohorts (made up of 15 SWX equities separated by liquidity level) separately before proceeding to the final merge into one dataset. The process is separated into code blocks as each merge takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = concat_excel(os.path.join('..', 'data', 'raw', 'high'))\n",
    "df.to_csv(os.path.join('..', 'data', 'interim', 'prelim merge', 'merge1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = concat_excel(os.path.join('..', 'data', 'raw', 'medium'))\n",
    "df.to_csv(os.path.join('..', 'data', 'interim', 'prelim merge', 'merge2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = concat_excel(os.path.join('..', 'data', 'raw', 'low'))\n",
    "df.to_csv(os.path.join('..', 'data', 'interim', 'prelim merge', 'merge3.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diana\\AppData\\Local\\Temp\\ipykernel_22304\\3930178371.py:6: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Diana\\AppData\\Local\\Temp\\ipykernel_22304\\3930178371.py:6: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Diana\\AppData\\Local\\Temp\\ipykernel_22304\\3930178371.py:6: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0      RIC                 Timestamp    Tick  Last Trade  Volume  \\\n",
      "0           0   UBSG.S  29-Jan-2024 17:40:05.746     NaN         NaN     NaN   \n",
      "1           1   NESN.S  29-Jan-2024 17:40:05.743     NaN         NaN     NaN   \n",
      "2           2  SRENH.S  29-Jan-2024 17:40:05.742     NaN         NaN     NaN   \n",
      "3           3   ABBN.S  29-Jan-2024 17:40:05.742     NaN         NaN     NaN   \n",
      "4           4    KUD.S  29-Jan-2024 17:40:05.736     NaN         NaN     NaN   \n",
      "5           5   NESN.S  29-Jan-2024 17:39:23.360    UP ↑       99.39    35.0   \n",
      "6           6   ABBN.S  29-Jan-2024 17:39:07.293  DOWN ↓       36.95   397.0   \n",
      "7           7   NESN.S  29-Jan-2024 17:39:02.740    UP ↑       99.39    10.0   \n",
      "8           8   NESN.S  29-Jan-2024 17:34:35.421    UP ↑       99.39   120.0   \n",
      "9           9  SRENH.S  29-Jan-2024 17:34:35.253  DOWN ↓       99.28   989.0   \n",
      "\n",
      "      VWAP  Bid Size    Bid    Ask  Ask Size  Turnover          Flow  \\\n",
      "0      NaN       NaN    NaN    NaN       NaN       NaN -1.338239e+09   \n",
      "1      NaN       NaN    NaN    NaN       NaN       NaN  5.423357e+08   \n",
      "2      NaN       NaN    NaN    NaN       NaN       NaN  1.645495e+09   \n",
      "3      NaN       NaN    NaN    NaN       NaN       NaN -5.991242e+09   \n",
      "4      NaN       NaN    NaN    NaN       NaN       NaN  6.682585e+03   \n",
      "5  99.0999     314.0  99.36  99.40    4210.0   3478.65  5.423357e+08   \n",
      "6  36.7975    5615.0  36.95  36.96    7099.0  14669.15 -5.991242e+09   \n",
      "7  99.0999     314.0  99.36  99.40    4210.0    993.90  5.423323e+08   \n",
      "8  99.0999     314.0  99.36  99.40    4210.0  11926.80  5.423313e+08   \n",
      "9  99.5181    7151.0  99.28  99.34     119.0  98187.92  1.645495e+09   \n",
      "\n",
      "   Calc VWAP Venue  \n",
      "0  25.680453   NaN  \n",
      "1  98.881887   NaN  \n",
      "2  99.157855   NaN  \n",
      "3  36.979356   NaN  \n",
      "4   1.229876   NaN  \n",
      "5  98.881887   NaN  \n",
      "6  36.979356   NaN  \n",
      "7  98.881887   NaN  \n",
      "8  98.881887   NaN  \n",
      "9  99.157855   NaN  \n"
     ]
    }
   ],
   "source": [
    "dir = os.path.join('..', 'data', 'interim', 'prelim merge')\n",
    "merged_df = pd.DataFrame()\n",
    "for file in os.listdir(dir):        \n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(dir, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "merged_df.to_csv(os.path.join('..', 'data', 'interim', 'merged.csv'), index=False)\n",
    "print(merged_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "We clean the data by removing unnecessary columns and values and deleting rows that have NaN values in 'Last Trade' and/or 'Volume'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and keep desired columns\n",
    "columns = ['RIC', 'Timestamp', 'Tick', 'Last Trade', 'Volume']\n",
    "final_df = merged_df[columns].copy()\n",
    "\n",
    "# remove unnecessary arrow sign in 'Tick' and null values\n",
    "final_df['Tick'] =final_df['Tick'].astype(str).str[:-2]\n",
    "final_df.dropna(subset=['Last Trade', 'Volume'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates\n",
    "\n",
    "We check for duplicates in the columns 'RIC' and 'Timestamp' which indicate simultaneously executed trades and keep the last occurence in each series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates found: 902738\n"
     ]
    }
   ],
   "source": [
    "duplicates = final_df.duplicated(subset=['RIC', 'Timestamp'], keep='last')\n",
    "\n",
    "# count duplicates\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicates found: {num_duplicates}\")\n",
    "\n",
    "final_df.drop_duplicates(subset=['RIC', 'Timestamp'], keep='last', inplace=True)\n",
    "final_df.to_csv(os.path.join('..', 'data', 'processed', 'tick_data.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riskmanagement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
